{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNFb/L3xrW3fmEDXqr+9KCs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/qortjdbs/ISEF_SullivanVision/blob/main/FER_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "lIviIcc0D2w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip /content/drive/MyDrive/ISEF/dataset2.zip"
      ],
      "metadata": {
        "id": "8y2261-5492n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnWUCP6v44am"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from platform import python_version\n",
        "import warnings\n",
        "import time\n",
        "import datetime as dtw\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import multiprocessing as mp\n",
        "import shutil\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input, decode_predictions\n",
        "from tensorflow.keras.applications import InceptionV3, ResNet50, Xception\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.utils import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras.initializers import *\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.initializers import glorot_uniform\n",
        "from keras.models import Model\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET\n",
        "import psutil\n",
        "import random\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting"
      ],
      "metadata": {
        "id": "S2Vr4RVyD9Lx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 48\n",
        "testsplit = .2\n",
        "targetx = 96\n",
        "targety = 96\n",
        "learning_rate = 0.0001\n",
        "classes = 7\n",
        "seed = 23\n",
        "\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    brightness_range=[0.9, 1.1],\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "a-xeIx_45JuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "uNwsKhdKECb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing & Augumentation"
      ],
      "metadata": {
        "id": "fAts-1f9HKfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# CLAHE 적용\n",
        "def apply_clahe(img):\n",
        "    # 이미지 변환을 위해 LAB 색상 공간으로 변환\n",
        "    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
        "    l, a, b = cv2.split(lab)\n",
        "\n",
        "    # CLAHE 객체 생성 및 L 채널에 적용\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    cl = clahe.apply(l)\n",
        "\n",
        "    # 병합 및 RGB 색상 공간으로 변환\n",
        "    limg = cv2.merge((cl, a, b))\n",
        "    final = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
        "    return final\n",
        "\n",
        "# 고급 전처리 적용\n",
        "def advanced_preprocessing(img):\n",
        "    img = apply_clahe(img)\n",
        "    img = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "    return img\n",
        "\n",
        "# 데이터 증강 및 전처리 설정\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    brightness_range=[0.9, 1.1],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    fill_mode='nearest',\n",
        "    preprocessing_function=advanced_preprocessing,\n",
        ")"
      ],
      "metadata": {
        "id": "Dn3oQ7OUdyNB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augument and Save Images"
      ],
      "metadata": {
        "id": "ignuTN2rHNVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_and_save_images(data_dir, save_dir, datagen, num_augmentations=5):\n",
        "    for subdir, dirs, files in os.walk(data_dir):\n",
        "        for file in files:\n",
        "            img_path = os.path.join(subdir, file)\n",
        "            img = image.load_img(img_path, target_size=(targetx, targety))  # Ensure dimensions match your requirements\n",
        "            x = image.img_to_array(img)\n",
        "            x = np.expand_dims(x, axis=0)\n",
        "\n",
        "            # Define a saving directory for this image's augmentations\n",
        "            save_prefix = os.path.join(save_dir, os.path.basename(subdir))\n",
        "            if not os.path.exists(save_prefix):\n",
        "                os.makedirs(save_prefix)\n",
        "\n",
        "            # Create and save augmented images\n",
        "            i = 0\n",
        "            for batch in datagen.flow(x, batch_size=1, save_to_dir=save_prefix, save_prefix='aug', save_format='jpeg'):\n",
        "                i += 1\n",
        "                if i >= num_augmentations:\n",
        "                    break  # This creates 'num_augmentations' new images per original image\n",
        "\n",
        "data_dir = '/content/dataset'  # The directory of your original dataset\n",
        "save_dir = '/content/augmented_dataset'  # Where to save the augmented images\n",
        "num_augmentations = 5  # Number of augmented images to create per original image\n",
        "\n",
        "augment_and_save_images(data_dir, save_dir, datagen, num_augmentations)\n"
      ],
      "metadata": {
        "id": "3nc6vJmdeE4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generators"
      ],
      "metadata": {
        "id": "qlRtBB5VeOi2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and validation data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    brightness_range=[0.9, 1.1],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    fill_mode='nearest',\n",
        "    validation_split=testsplit,\n",
        "    preprocessing_function=advanced_preprocessing,\n",
        ")\n",
        "\n",
        "# Training generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    save_dir,\n",
        "    target_size=(targetx, targety),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=True,\n",
        "    seed=seed,\n",
        "    subset=\"training\"\n",
        ")\n",
        "\n",
        "# Validation generator\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    save_dir,\n",
        "    target_size=(targetx, targety),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False,\n",
        "    seed=seed,\n",
        "    subset=\"validation\",\n",
        ")"
      ],
      "metadata": {
        "id": "PqHCGZLBeRoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Xo91AHtkEG1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = ModelCheckpoint('FER.h5',\n",
        "                             monitor='val_accuracy',\n",
        "                             save_best_only=True,\n",
        "                             verbose=1,\n",
        "                             mode='auto',\n",
        "                             save_weights_only=False,\n",
        "                             period=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm_aCAHU5SYM",
        "outputId": "2eaee812-3b3f-4019-f403-7548a2db025b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## InceptionV3"
      ],
      "metadata": {
        "id": "AjNxlUJWERcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = InceptionV3(include_top=False, weights='imagenet', input_shape=(targetx, targety, 3))\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(512, activation='relu', kernel_initializer=glorot_uniform(seed), kernel_regularizer=regularizers.L2(0.001), activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(256, activation='relu', kernel_initializer=glorot_uniform(seed), kernel_regularizer=regularizers.L2(0.001), activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(128, activation='relu', kernel_initializer=glorot_uniform(seed), kernel_regularizer=regularizers.L2(0.001), activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(classes, activation='softmax', kernel_initializer='random_uniform', bias_initializer='zeros')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "loss = \"categorical_crossentropy\"\n",
        "\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "YELFbr0tWVM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DenseNet"
      ],
      "metadata": {
        "id": "4EY8XhE8EK_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = DenseNet121(include_top=False, weights='imagenet', input_shape=(targetx, targety, 3))\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Flatten() (x)\n",
        "x = BatchNormalization()(x)\n",
        "x=Dropout(0.3)(x)\n",
        "x = Dense(512, activation='relu',  kernel_initializer=glorot_uniform(seed),kernel_regularizer=regularizers.L2(0.001),activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x=Dropout(0.3)(x)\n",
        "x = Dense(256, activation='relu',  kernel_initializer=glorot_uniform(seed),kernel_regularizer=regularizers.L2(0.001),activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x=Dropout(0.3)(x)\n",
        "x = Dense(128, activation='relu',  kernel_initializer=glorot_uniform(seed),kernel_regularizer=regularizers.L2(0.001),activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x=Dropout(0.3)(x)\n",
        "predictions = Dense(classes, activation='softmax', kernel_initializer='random_uniform', bias_initializer='zeros')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "loss = \"categorical_crossentropy\"\n",
        "\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcib0PBR5Tjf",
        "outputId": "8790e977-e0be-4644-a751-5cefeee183f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29084464/29084464 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet50"
      ],
      "metadata": {
        "id": "6oegOpOGFt1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(targetx, targety, 3))\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Flatten() (x)\n",
        "x = BatchNormalization()(x)\n",
        "x=Dropout(0.3)(x)\n",
        "x = Dense(512, activation='relu',  kernel_initializer=glorot_uniform(seed),kernel_regularizer=regularizers.L2(0.001),activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x=Dropout(0.3)(x)\n",
        "x = Dense(256, activation='relu',  kernel_initializer=glorot_uniform(seed),kernel_regularizer=regularizers.L2(0.001),activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x=Dropout(0.3)(x)\n",
        "x = Dense(128, activation='relu',  kernel_initializer=glorot_uniform(seed),kernel_regularizer=regularizers.L2(0.001),activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x=Dropout(0.3)(x)\n",
        "predictions = Dense(classes, activation='softmax', kernel_initializer='random_uniform', bias_initializer='zeros')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "loss = \"categorical_crossentropy\"\n",
        "\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "SGTHujcGFxlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xception"
      ],
      "metadata": {
        "id": "AR0m3TUJEw-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Xception\n",
        "\n",
        "base_model = Xception(include_top=False, weights='imagenet', input_shape=(targetx, targety, 3))\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Flatten()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(512, activation='relu', kernel_initializer=glorot_uniform(seed), kernel_regularizer=regularizers.L2(0.001), activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(256, activation='relu', kernel_initializer=glorot_uniform(seed), kernel_regularizer=regularizers.L2(0.001), activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(128, activation='relu', kernel_initializer=glorot_uniform(seed), kernel_regularizer=regularizers.L2(0.001), activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(classes, activation='softmax', kernel_initializer='random_uniform', bias_initializer='zeros')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "loss = \"categorical_crossentropy\"\n",
        "\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC1Lvg9iWKLs",
        "outputId": "8e4c18c2-866b-4d00-a82f-f94eef78811c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83683744/83683744 [==============================] - 4s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Xception-AdCorre"
      ],
      "metadata": {
        "id": "yUnwE5RYEz-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Xception + Ad-Corre\n",
        "\n",
        "class AdCorreLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, num_classes, lambda_param=0.5, **kwargs):\n",
        "        super(AdCorreLoss, self).__init__(**kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.lambda_param = lambda_param\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Standard categorical cross-entropy loss\n",
        "        ce_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
        "\n",
        "        # Calculate the Ad-Corre components\n",
        "        fd_loss = self.feature_discriminator_loss(y_true, y_pred)\n",
        "        md_loss = self.mean_discriminator_loss(y_true, y_pred)\n",
        "        ed_loss = self.embedding_discriminator_loss(y_pred)\n",
        "\n",
        "        # Total Ad-Corre loss\n",
        "        ad_corre_loss = ce_loss + self.lambda_param * (fd_loss + md_loss + ed_loss)\n",
        "        return ad_corre_loss\n",
        "\n",
        "    def feature_discriminator_loss(self, y_true, y_pred):\n",
        "        # Compute correlation matrix for the feature discriminator\n",
        "        corr_matrix = self.compute_correlation_matrix(y_pred)\n",
        "\n",
        "        # Create npSign matrix based on true labels\n",
        "        np_sign = tf.sign(tf.tensordot(y_true, tf.transpose(y_true), axes=1))\n",
        "\n",
        "        # Compute Feature Discriminator Loss\n",
        "        fd_loss = tf.reduce_mean(tf.abs(np_sign - corr_matrix))\n",
        "        return fd_loss\n",
        "\n",
        "    def mean_discriminator_loss(self, y_true, y_pred):\n",
        "        # Compute mean of embedded feature vectors for each class\n",
        "        mean_vectors = tf.tensordot(y_true, y_pred, axes=1) / tf.reduce_sum(y_true, axis=0, keepdims=True)\n",
        "\n",
        "        # Compute correlation matrix for the mean vectors\n",
        "        corr_matrix = self.compute_correlation_matrix(mean_vectors)\n",
        "\n",
        "        # Compute Mean Discriminator Loss\n",
        "        md_loss = tf.reduce_sum(tf.abs(1 + corr_matrix)) / self.num_classes\n",
        "        return md_loss\n",
        "\n",
        "    def embedding_discriminator_loss(self, y_pred):\n",
        "        # Compute correlation matrix for embedded feature vectors\n",
        "        corr_matrix = self.compute_correlation_matrix(y_pred)\n",
        "\n",
        "        # Compute Embedding Discriminator Loss\n",
        "        ed_loss = tf.reduce_sum(tf.abs(1 + corr_matrix)) / y_pred.shape[1]\n",
        "        return ed_loss\n",
        "\n",
        "    def compute_correlation_matrix(self, vectors):\n",
        "        # Subtract mean from vectors\n",
        "        mean = tf.reduce_mean(vectors, axis=0, keepdims=True)\n",
        "        vectors_centered = vectors - mean\n",
        "\n",
        "        # Compute covariance\n",
        "        covariance = tf.matmul(vectors_centered, vectors_centered, transpose_a=True)\n",
        "\n",
        "        # Compute variance\n",
        "        variance = tf.linalg.diag_part(covariance)\n",
        "\n",
        "        # Compute correlation matrix\n",
        "        correlation_matrix = covariance / (tf.sqrt(variance[:, None]) * tf.sqrt(variance[None, :]) + 1e-8)\n",
        "        return correlation_matrix\n",
        "\n",
        "# Model architecture\n",
        "def build_model(input_shape, num_classes, learning_rate, seed):\n",
        "    base_model = Xception(include_top=False, weights='imagenet', input_shape=input_shape)\n",
        "\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(512, activation='relu', kernel_initializer=glorot_uniform(seed),\n",
        "              kernel_regularizer=regularizers.L2(0.001),\n",
        "              activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(256, activation='relu', kernel_initializer=glorot_uniform(seed),\n",
        "              kernel_regularizer=regularizers.L2(0.001),\n",
        "              activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(128, activation='relu', kernel_initializer=glorot_uniform(seed),\n",
        "              kernel_regularizer=regularizers.L2(0.001),\n",
        "              activity_regularizer=regularizers.L2(1e-4), bias_initializer='zeros')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    predictions = Dense(num_classes, activation='softmax',\n",
        "                        kernel_initializer='random_uniform', bias_initializer='zeros')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Compile model with Ad-Corre Loss\n",
        "    ad_corre_loss = AdCorreLoss(num_classes=num_classes)\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=ad_corre_loss, metrics=[\"accuracy\"])\n",
        "\n",
        "    # Make all layers trainable\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = True\n",
        "\n",
        "    return model\n",
        "\n",
        "# Usage example\n",
        "input_shape = (224, 224, 3)  # Define your input shape\n",
        "num_classes = 7  # Define the number of classes\n",
        "learning_rate = 0.001\n",
        "seed = 42\n",
        "\n",
        "model_Xception_AdCorre = build_model(input_shape, num_classes, learning_rate, seed)\n",
        "\n",
        "# Model summary\n",
        "model_Xception_AdCorre.summary()\n"
      ],
      "metadata": {
        "id": "jRjszAp7CxPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "1FvXSG00FGGx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Process"
      ],
      "metadata": {
        "id": "SjkcTKEBHbCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "\n",
        "model_history = model_Xception_AdCorre.fit_generator(generator=train_generator,\n",
        "                                steps_per_epoch=len(train_generator),\n",
        "                                validation_data=val_generator,\n",
        "                                validation_steps=len(val_generator),\n",
        "                                epochs=epochs,\n",
        "                                callbacks=[checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQXi-cd95U72",
        "outputId": "af15253c-e025-4caa-97f1-b3bc7e7cf9d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 3.0232 - accuracy: 0.3288\n",
            "Epoch 1: val_accuracy improved from -inf to 0.17891, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 48s 222ms/step - loss: 3.0232 - accuracy: 0.3288 - val_loss: 3.1229 - val_accuracy: 0.1789\n",
            "Epoch 2/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.5009 - accuracy: 0.4495\n",
            "Epoch 2: val_accuracy did not improve from 0.17891\n",
            "79/79 [==============================] - 15s 195ms/step - loss: 2.5009 - accuracy: 0.4495 - val_loss: 2.9767 - val_accuracy: 0.1374\n",
            "Epoch 3/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 2.0213 - accuracy: 0.6049\n",
            "Epoch 3: val_accuracy did not improve from 0.17891\n",
            "79/79 [==============================] - 15s 194ms/step - loss: 2.0213 - accuracy: 0.6049 - val_loss: 3.1166 - val_accuracy: 0.1789\n",
            "Epoch 4/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 1.5645 - accuracy: 0.7160\n",
            "Epoch 4: val_accuracy did not improve from 0.17891\n",
            "79/79 [==============================] - 15s 193ms/step - loss: 1.5645 - accuracy: 0.7160 - val_loss: 3.0443 - val_accuracy: 0.1384\n",
            "Epoch 5/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 1.3270 - accuracy: 0.7589\n",
            "Epoch 5: val_accuracy did not improve from 0.17891\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 1.3270 - accuracy: 0.7589 - val_loss: 3.9358 - val_accuracy: 0.1374\n",
            "Epoch 6/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 1.1929 - accuracy: 0.7826\n",
            "Epoch 6: val_accuracy improved from 0.17891 to 0.42918, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 204ms/step - loss: 1.1929 - accuracy: 0.7826 - val_loss: 2.0454 - val_accuracy: 0.4292\n",
            "Epoch 7/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 1.0692 - accuracy: 0.8043\n",
            "Epoch 7: val_accuracy did not improve from 0.42918\n",
            "79/79 [==============================] - 15s 195ms/step - loss: 1.0692 - accuracy: 0.8043 - val_loss: 11.0762 - val_accuracy: 0.4100\n",
            "Epoch 8/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.9644 - accuracy: 0.8149\n",
            "Epoch 8: val_accuracy did not improve from 0.42918\n",
            "79/79 [==============================] - 15s 194ms/step - loss: 0.9644 - accuracy: 0.8149 - val_loss: 2.8946 - val_accuracy: 0.3834\n",
            "Epoch 9/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.8842 - accuracy: 0.8390\n",
            "Epoch 9: val_accuracy improved from 0.42918 to 0.76784, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 208ms/step - loss: 0.8842 - accuracy: 0.8390 - val_loss: 1.1198 - val_accuracy: 0.7678\n",
            "Epoch 10/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.7759 - accuracy: 0.8717\n",
            "Epoch 10: val_accuracy improved from 0.76784 to 0.81470, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 202ms/step - loss: 0.7759 - accuracy: 0.8717 - val_loss: 0.9850 - val_accuracy: 0.8147\n",
            "Epoch 11/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.6748 - accuracy: 0.8899\n",
            "Epoch 11: val_accuracy improved from 0.81470 to 0.84878, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 204ms/step - loss: 0.6748 - accuracy: 0.8899 - val_loss: 0.7173 - val_accuracy: 0.8488\n",
            "Epoch 12/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.6559 - accuracy: 0.8873\n",
            "Epoch 12: val_accuracy improved from 0.84878 to 0.89670, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 204ms/step - loss: 0.6559 - accuracy: 0.8873 - val_loss: 0.6386 - val_accuracy: 0.8967\n",
            "Epoch 13/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.6231 - accuracy: 0.8913\n",
            "Epoch 13: val_accuracy did not improve from 0.89670\n",
            "79/79 [==============================] - 15s 189ms/step - loss: 0.6231 - accuracy: 0.8913 - val_loss: 3.7327 - val_accuracy: 0.8935\n",
            "Epoch 14/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.5568 - accuracy: 0.8992\n",
            "Epoch 14: val_accuracy did not improve from 0.89670\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.5568 - accuracy: 0.8992 - val_loss: 1.4837 - val_accuracy: 0.8605\n",
            "Epoch 15/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4936 - accuracy: 0.9196\n",
            "Epoch 15: val_accuracy improved from 0.89670 to 0.90841, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 207ms/step - loss: 0.4936 - accuracy: 0.9196 - val_loss: 0.5927 - val_accuracy: 0.9084\n",
            "Epoch 16/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4893 - accuracy: 0.9183\n",
            "Epoch 16: val_accuracy did not improve from 0.90841\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.4893 - accuracy: 0.9183 - val_loss: 0.8170 - val_accuracy: 0.8275\n",
            "Epoch 17/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4399 - accuracy: 0.9263\n",
            "Epoch 17: val_accuracy improved from 0.90841 to 0.91906, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 204ms/step - loss: 0.4399 - accuracy: 0.9263 - val_loss: 0.6090 - val_accuracy: 0.9191\n",
            "Epoch 18/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.9332\n",
            "Epoch 18: val_accuracy improved from 0.91906 to 0.93610, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 202ms/step - loss: 0.4117 - accuracy: 0.9332 - val_loss: 0.3665 - val_accuracy: 0.9361\n",
            "Epoch 19/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4051 - accuracy: 0.9334\n",
            "Epoch 19: val_accuracy did not improve from 0.93610\n",
            "79/79 [==============================] - 15s 189ms/step - loss: 0.4051 - accuracy: 0.9334 - val_loss: 0.4862 - val_accuracy: 0.9042\n",
            "Epoch 20/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.4094 - accuracy: 0.9316\n",
            "Epoch 20: val_accuracy did not improve from 0.93610\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.4094 - accuracy: 0.9316 - val_loss: 1.6484 - val_accuracy: 0.8956\n",
            "Epoch 21/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.3531 - accuracy: 0.9419\n",
            "Epoch 21: val_accuracy did not improve from 0.93610\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.3531 - accuracy: 0.9419 - val_loss: 0.4106 - val_accuracy: 0.9308\n",
            "Epoch 22/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.3671 - accuracy: 0.9401\n",
            "Epoch 22: val_accuracy did not improve from 0.93610\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.3671 - accuracy: 0.9401 - val_loss: 20.1215 - val_accuracy: 0.8158\n",
            "Epoch 23/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.3405 - accuracy: 0.9456\n",
            "Epoch 23: val_accuracy did not improve from 0.93610\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.3405 - accuracy: 0.9456 - val_loss: 0.4770 - val_accuracy: 0.9350\n",
            "Epoch 24/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.3600 - accuracy: 0.9395\n",
            "Epoch 24: val_accuracy improved from 0.93610 to 0.93823, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 17s 209ms/step - loss: 0.3600 - accuracy: 0.9395 - val_loss: 0.4511 - val_accuracy: 0.9382\n",
            "Epoch 25/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.3562 - accuracy: 0.9446\n",
            "Epoch 25: val_accuracy improved from 0.93823 to 0.93930, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 202ms/step - loss: 0.3562 - accuracy: 0.9446 - val_loss: 0.4705 - val_accuracy: 0.9393\n",
            "Epoch 26/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.3391 - accuracy: 0.9446\n",
            "Epoch 26: val_accuracy did not improve from 0.93930\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.3391 - accuracy: 0.9446 - val_loss: 0.6828 - val_accuracy: 0.8552\n",
            "Epoch 27/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.3819 - accuracy: 0.9329\n",
            "Epoch 27: val_accuracy did not improve from 0.93930\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.3819 - accuracy: 0.9329 - val_loss: 0.8503 - val_accuracy: 0.8158\n",
            "Epoch 28/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.3180 - accuracy: 0.9483\n",
            "Epoch 28: val_accuracy did not improve from 0.93930\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.3180 - accuracy: 0.9483 - val_loss: 1.5799 - val_accuracy: 0.9116\n",
            "Epoch 29/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2854 - accuracy: 0.9536\n",
            "Epoch 29: val_accuracy improved from 0.93930 to 0.95740, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 203ms/step - loss: 0.2854 - accuracy: 0.9536 - val_loss: 0.5291 - val_accuracy: 0.9574\n",
            "Epoch 30/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2698 - accuracy: 0.9570\n",
            "Epoch 30: val_accuracy did not improve from 0.95740\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.2698 - accuracy: 0.9570 - val_loss: 1.0533 - val_accuracy: 0.9297\n",
            "Epoch 31/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2831 - accuracy: 0.9552\n",
            "Epoch 31: val_accuracy did not improve from 0.95740\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.2831 - accuracy: 0.9552 - val_loss: 0.3299 - val_accuracy: 0.9436\n",
            "Epoch 32/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.3029 - accuracy: 0.9501\n",
            "Epoch 32: val_accuracy did not improve from 0.95740\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.3029 - accuracy: 0.9501 - val_loss: 0.3132 - val_accuracy: 0.9489\n",
            "Epoch 33/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9549\n",
            "Epoch 33: val_accuracy did not improve from 0.95740\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.2576 - accuracy: 0.9549 - val_loss: 0.5291 - val_accuracy: 0.9340\n",
            "Epoch 34/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2729 - accuracy: 0.9525\n",
            "Epoch 34: val_accuracy did not improve from 0.95740\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.2729 - accuracy: 0.9525 - val_loss: 0.5165 - val_accuracy: 0.9159\n",
            "Epoch 35/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9592\n",
            "Epoch 35: val_accuracy did not improve from 0.95740\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.2647 - accuracy: 0.9592 - val_loss: 3.5254 - val_accuracy: 0.9116\n",
            "Epoch 36/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2998 - accuracy: 0.9475\n",
            "Epoch 36: val_accuracy did not improve from 0.95740\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.2998 - accuracy: 0.9475 - val_loss: 0.4415 - val_accuracy: 0.9223\n",
            "Epoch 37/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2932 - accuracy: 0.9509\n",
            "Epoch 37: val_accuracy did not improve from 0.95740\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.2932 - accuracy: 0.9509 - val_loss: 0.3174 - val_accuracy: 0.9542\n",
            "Epoch 38/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.9544\n",
            "Epoch 38: val_accuracy did not improve from 0.95740\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.2852 - accuracy: 0.9544 - val_loss: 0.4399 - val_accuracy: 0.9084\n",
            "Epoch 39/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2503 - accuracy: 0.9594\n",
            "Epoch 39: val_accuracy improved from 0.95740 to 0.96166, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 205ms/step - loss: 0.2503 - accuracy: 0.9594 - val_loss: 0.2157 - val_accuracy: 0.9617\n",
            "Epoch 40/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2340 - accuracy: 0.9615\n",
            "Epoch 40: val_accuracy did not improve from 0.96166\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.2340 - accuracy: 0.9615 - val_loss: 0.4744 - val_accuracy: 0.8946\n",
            "Epoch 41/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2326 - accuracy: 0.9669\n",
            "Epoch 41: val_accuracy improved from 0.96166 to 0.97018, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 203ms/step - loss: 0.2326 - accuracy: 0.9669 - val_loss: 0.3126 - val_accuracy: 0.9702\n",
            "Epoch 42/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2345 - accuracy: 0.9645\n",
            "Epoch 42: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 194ms/step - loss: 0.2345 - accuracy: 0.9645 - val_loss: 0.3466 - val_accuracy: 0.9212\n",
            "Epoch 43/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2241 - accuracy: 0.9679\n",
            "Epoch 43: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.2241 - accuracy: 0.9679 - val_loss: 0.2424 - val_accuracy: 0.9627\n",
            "Epoch 44/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.9613\n",
            "Epoch 44: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.2549 - accuracy: 0.9613 - val_loss: 0.3781 - val_accuracy: 0.9510\n",
            "Epoch 45/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2270 - accuracy: 0.9690\n",
            "Epoch 45: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.2270 - accuracy: 0.9690 - val_loss: 0.6430 - val_accuracy: 0.9404\n",
            "Epoch 46/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2126 - accuracy: 0.9682\n",
            "Epoch 46: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 196ms/step - loss: 0.2126 - accuracy: 0.9682 - val_loss: 0.3564 - val_accuracy: 0.9446\n",
            "Epoch 47/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.9690\n",
            "Epoch 47: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.2138 - accuracy: 0.9690 - val_loss: 0.2242 - val_accuracy: 0.9585\n",
            "Epoch 48/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.9684\n",
            "Epoch 48: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 193ms/step - loss: 0.2138 - accuracy: 0.9684 - val_loss: 0.6511 - val_accuracy: 0.9127\n",
            "Epoch 49/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2410 - accuracy: 0.9613\n",
            "Epoch 49: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 194ms/step - loss: 0.2410 - accuracy: 0.9613 - val_loss: 0.3355 - val_accuracy: 0.9499\n",
            "Epoch 50/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.9647\n",
            "Epoch 50: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 194ms/step - loss: 0.2464 - accuracy: 0.9647 - val_loss: 0.3576 - val_accuracy: 0.9468\n",
            "Epoch 51/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1945 - accuracy: 0.9730\n",
            "Epoch 51: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.1945 - accuracy: 0.9730 - val_loss: 0.2561 - val_accuracy: 0.9681\n",
            "Epoch 52/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2149 - accuracy: 0.9684\n",
            "Epoch 52: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 193ms/step - loss: 0.2149 - accuracy: 0.9684 - val_loss: 1.1887 - val_accuracy: 0.9574\n",
            "Epoch 53/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2168 - accuracy: 0.9724\n",
            "Epoch 53: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.2168 - accuracy: 0.9724 - val_loss: 1.4113 - val_accuracy: 0.9478\n",
            "Epoch 54/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2401 - accuracy: 0.9639\n",
            "Epoch 54: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.2401 - accuracy: 0.9639 - val_loss: 0.3897 - val_accuracy: 0.9414\n",
            "Epoch 55/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2020 - accuracy: 0.9751\n",
            "Epoch 55: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.2020 - accuracy: 0.9751 - val_loss: 0.3939 - val_accuracy: 0.9095\n",
            "Epoch 56/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.9722\n",
            "Epoch 56: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 194ms/step - loss: 0.1955 - accuracy: 0.9722 - val_loss: 0.4061 - val_accuracy: 0.9510\n",
            "Epoch 57/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1997 - accuracy: 0.9753\n",
            "Epoch 57: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 193ms/step - loss: 0.1997 - accuracy: 0.9753 - val_loss: 0.2413 - val_accuracy: 0.9617\n",
            "Epoch 58/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9751\n",
            "Epoch 58: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.1934 - accuracy: 0.9751 - val_loss: 0.3613 - val_accuracy: 0.9521\n",
            "Epoch 59/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2098 - accuracy: 0.9714\n",
            "Epoch 59: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 194ms/step - loss: 0.2098 - accuracy: 0.9714 - val_loss: 0.3184 - val_accuracy: 0.9425\n",
            "Epoch 60/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2124 - accuracy: 0.9708\n",
            "Epoch 60: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.2124 - accuracy: 0.9708 - val_loss: 0.4782 - val_accuracy: 0.9638\n",
            "Epoch 61/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2116 - accuracy: 0.9682\n",
            "Epoch 61: val_accuracy did not improve from 0.97018\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.2116 - accuracy: 0.9682 - val_loss: 0.2012 - val_accuracy: 0.9702\n",
            "Epoch 62/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.9756\n",
            "Epoch 62: val_accuracy improved from 0.97018 to 0.97870, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 202ms/step - loss: 0.1965 - accuracy: 0.9756 - val_loss: 0.2473 - val_accuracy: 0.9787\n",
            "Epoch 63/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2145 - accuracy: 0.9703\n",
            "Epoch 63: val_accuracy did not improve from 0.97870\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.2145 - accuracy: 0.9703 - val_loss: 0.2970 - val_accuracy: 0.9553\n",
            "Epoch 64/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2045 - accuracy: 0.9735\n",
            "Epoch 64: val_accuracy improved from 0.97870 to 0.98616, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 205ms/step - loss: 0.2045 - accuracy: 0.9735 - val_loss: 0.1458 - val_accuracy: 0.9862\n",
            "Epoch 65/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1677 - accuracy: 0.9820\n",
            "Epoch 65: val_accuracy did not improve from 0.98616\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.1677 - accuracy: 0.9820 - val_loss: 0.1973 - val_accuracy: 0.9723\n",
            "Epoch 66/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1655 - accuracy: 0.9796\n",
            "Epoch 66: val_accuracy did not improve from 0.98616\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.1655 - accuracy: 0.9796 - val_loss: 0.1986 - val_accuracy: 0.9702\n",
            "Epoch 67/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1679 - accuracy: 0.9814\n",
            "Epoch 67: val_accuracy did not improve from 0.98616\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.1679 - accuracy: 0.9814 - val_loss: 0.2980 - val_accuracy: 0.9638\n",
            "Epoch 68/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1848 - accuracy: 0.9759\n",
            "Epoch 68: val_accuracy did not improve from 0.98616\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.1848 - accuracy: 0.9759 - val_loss: 1.8278 - val_accuracy: 0.8711\n",
            "Epoch 69/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2101 - accuracy: 0.9695\n",
            "Epoch 69: val_accuracy did not improve from 0.98616\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.2101 - accuracy: 0.9695 - val_loss: 0.4659 - val_accuracy: 0.9404\n",
            "Epoch 70/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.9626\n",
            "Epoch 70: val_accuracy did not improve from 0.98616\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.2426 - accuracy: 0.9626 - val_loss: 0.5643 - val_accuracy: 0.9425\n",
            "Epoch 71/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.9780\n",
            "Epoch 71: val_accuracy did not improve from 0.98616\n",
            "79/79 [==============================] - 15s 195ms/step - loss: 0.1970 - accuracy: 0.9780 - val_loss: 1.9666 - val_accuracy: 0.9265\n",
            "Epoch 72/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1872 - accuracy: 0.9793\n",
            "Epoch 72: val_accuracy did not improve from 0.98616\n",
            "79/79 [==============================] - 15s 193ms/step - loss: 0.1872 - accuracy: 0.9793 - val_loss: 0.2174 - val_accuracy: 0.9787\n",
            "Epoch 73/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2157 - accuracy: 0.9703\n",
            "Epoch 73: val_accuracy did not improve from 0.98616\n",
            "79/79 [==============================] - 16s 196ms/step - loss: 0.2157 - accuracy: 0.9703 - val_loss: 0.5003 - val_accuracy: 0.9425\n",
            "Epoch 74/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.9761\n",
            "Epoch 74: val_accuracy did not improve from 0.98616\n",
            "79/79 [==============================] - 15s 189ms/step - loss: 0.1973 - accuracy: 0.9761 - val_loss: 0.3336 - val_accuracy: 0.9414\n",
            "Epoch 75/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2142 - accuracy: 0.9756\n",
            "Epoch 75: val_accuracy did not improve from 0.98616\n",
            "79/79 [==============================] - 15s 188ms/step - loss: 0.2142 - accuracy: 0.9756 - val_loss: 3.1109 - val_accuracy: 0.9553\n",
            "Epoch 76/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1718 - accuracy: 0.9809\n",
            "Epoch 76: val_accuracy did not improve from 0.98616\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.1718 - accuracy: 0.9809 - val_loss: 0.2110 - val_accuracy: 0.9723\n",
            "Epoch 77/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1954 - accuracy: 0.9751\n",
            "Epoch 77: val_accuracy did not improve from 0.98616\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.1954 - accuracy: 0.9751 - val_loss: 0.9031 - val_accuracy: 0.9734\n",
            "Epoch 78/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1841 - accuracy: 0.9798\n",
            "Epoch 78: val_accuracy improved from 0.98616 to 0.98935, saving model to model_isef2.h5\n",
            "79/79 [==============================] - 16s 203ms/step - loss: 0.1841 - accuracy: 0.9798 - val_loss: 0.6715 - val_accuracy: 0.9894\n",
            "Epoch 79/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.9820\n",
            "Epoch 79: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 196ms/step - loss: 0.1548 - accuracy: 0.9820 - val_loss: 0.4304 - val_accuracy: 0.9521\n",
            "Epoch 80/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1507 - accuracy: 0.9851\n",
            "Epoch 80: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 193ms/step - loss: 0.1507 - accuracy: 0.9851 - val_loss: 0.2689 - val_accuracy: 0.9840\n",
            "Epoch 81/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1793 - accuracy: 0.9791\n",
            "Epoch 81: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.1793 - accuracy: 0.9791 - val_loss: 0.2211 - val_accuracy: 0.9627\n",
            "Epoch 82/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1455 - accuracy: 0.9867\n",
            "Epoch 82: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 194ms/step - loss: 0.1455 - accuracy: 0.9867 - val_loss: 0.2706 - val_accuracy: 0.9510\n",
            "Epoch 83/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1724 - accuracy: 0.9820\n",
            "Epoch 83: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 193ms/step - loss: 0.1724 - accuracy: 0.9820 - val_loss: 0.2544 - val_accuracy: 0.9585\n",
            "Epoch 84/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1960 - accuracy: 0.9735\n",
            "Epoch 84: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 190ms/step - loss: 0.1960 - accuracy: 0.9735 - val_loss: 0.4893 - val_accuracy: 0.9478\n",
            "Epoch 85/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.9708\n",
            "Epoch 85: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 194ms/step - loss: 0.2155 - accuracy: 0.9708 - val_loss: 1.1315 - val_accuracy: 0.9084\n",
            "Epoch 86/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1807 - accuracy: 0.9759\n",
            "Epoch 86: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 195ms/step - loss: 0.1807 - accuracy: 0.9759 - val_loss: 0.3497 - val_accuracy: 0.9659\n",
            "Epoch 87/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1418 - accuracy: 0.9838\n",
            "Epoch 87: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 195ms/step - loss: 0.1418 - accuracy: 0.9838 - val_loss: 0.1590 - val_accuracy: 0.9776\n",
            "Epoch 88/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1606 - accuracy: 0.9817\n",
            "Epoch 88: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 193ms/step - loss: 0.1606 - accuracy: 0.9817 - val_loss: 0.2003 - val_accuracy: 0.9702\n",
            "Epoch 89/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1741 - accuracy: 0.9825\n",
            "Epoch 89: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.1741 - accuracy: 0.9825 - val_loss: 0.3977 - val_accuracy: 0.9595\n",
            "Epoch 90/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.9775\n",
            "Epoch 90: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 194ms/step - loss: 0.1915 - accuracy: 0.9775 - val_loss: 0.3303 - val_accuracy: 0.9627\n",
            "Epoch 91/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1802 - accuracy: 0.9796\n",
            "Epoch 91: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 194ms/step - loss: 0.1802 - accuracy: 0.9796 - val_loss: 0.2128 - val_accuracy: 0.9712\n",
            "Epoch 92/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1565 - accuracy: 0.9830\n",
            "Epoch 92: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 193ms/step - loss: 0.1565 - accuracy: 0.9830 - val_loss: 0.1645 - val_accuracy: 0.9798\n",
            "Epoch 93/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.9788\n",
            "Epoch 93: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 194ms/step - loss: 0.1773 - accuracy: 0.9788 - val_loss: 7.0621 - val_accuracy: 0.9286\n",
            "Epoch 94/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.9801\n",
            "Epoch 94: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 195ms/step - loss: 0.1667 - accuracy: 0.9801 - val_loss: 1.2494 - val_accuracy: 0.9414\n",
            "Epoch 95/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1833 - accuracy: 0.9737\n",
            "Epoch 95: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 192ms/step - loss: 0.1833 - accuracy: 0.9737 - val_loss: 0.4933 - val_accuracy: 0.8839\n",
            "Epoch 96/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9801\n",
            "Epoch 96: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 191ms/step - loss: 0.1571 - accuracy: 0.9801 - val_loss: 0.2411 - val_accuracy: 0.9542\n",
            "Epoch 97/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1623 - accuracy: 0.9804\n",
            "Epoch 97: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 16s 196ms/step - loss: 0.1623 - accuracy: 0.9804 - val_loss: 0.3146 - val_accuracy: 0.9361\n",
            "Epoch 98/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1514 - accuracy: 0.9833\n",
            "Epoch 98: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 194ms/step - loss: 0.1514 - accuracy: 0.9833 - val_loss: 0.1354 - val_accuracy: 0.9840\n",
            "Epoch 99/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1644 - accuracy: 0.9812\n",
            "Epoch 99: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 193ms/step - loss: 0.1644 - accuracy: 0.9812 - val_loss: 0.1872 - val_accuracy: 0.9691\n",
            "Epoch 100/100\n",
            "79/79 [==============================] - ETA: 0s - loss: 0.1912 - accuracy: 0.9761\n",
            "Epoch 100: val_accuracy did not improve from 0.98935\n",
            "79/79 [==============================] - 15s 195ms/step - loss: 0.1912 - accuracy: 0.9761 - val_loss: 0.2190 - val_accuracy: 0.9691\n",
            "CPU times: user 32min 37s, sys: 41.1 s, total: 33min 18s\n",
            "Wall time: 26min 9s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization"
      ],
      "metadata": {
        "id": "wVdRJWwEHhtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax=plt.subplots(figsize=(6,3))\n",
        "plt.plot(model_history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(model_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "sgU5K5v0B8mN",
        "outputId": "26fb2c9b-fdd9-4843-d084-f44510ac6c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}